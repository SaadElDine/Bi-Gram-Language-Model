# Bi-gram Model for Text Generation

This project implements a bi-gram model for text generation based on character sequences. The model is trained on a dataset of names and can generate new name-like sequences of characters.

![image](https://github.com/SaadElDine/Bi-Gram-Language-Model/assets/113860522/ea020a79-8634-4707-9e66-b4c33fec6bca)


# Model Details
- The model uses a bi-gram approach, calculating the transition probabilities between pairs of characters.
- Smoothing is applied to avoid zero probabilities.
- Text generation is done using a multinomial distribution based on the transition probabilities.
- Negative log-likelihood (NLL) and perplexity (PPL) are calculated to evaluate the model's performance.

# Sample output
junide.
janasah.
p.
cony.
a.
nn.
kohin.
tolian.
juee.
ksahnaauranilevias.
dedainrwieta.
ssonielylarte.
faveumerifontume.
phynslenaruani.
core.
yaenon.
ka.
jabdinerimikimaynin.
anaasn.
ssorionsush.
dgossmitan.
il.
le.
pann.
that.
janreli.
isa.
dyn.
rijelumemahaunayaleva.
cararr.
jen.
janarta.
maly.
abely.
a.
i.
lavadoni.
themielyawat.
f.
modam.
tavilitikiesaloeverin.
n.
e.
kalbrenelah.
anen.
ch.
k.
jan.
odridrdenanialilpergha.
tezralelia.
